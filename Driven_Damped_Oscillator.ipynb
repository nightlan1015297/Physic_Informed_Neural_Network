{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driven damped oscillation\n",
    "The equation of motion for the driven damped oscillation is given by:\n",
    "$$F_0 cos(\\omega{t}+\\phi) = m\\frac{d^2x}{dt^2}+\\gamma\\frac{dx}{dt}+kx$$\n",
    "\n",
    "From this equation, we can define the physics-informed loss as:\n",
    "$$(m\\frac{d^2x}{dt^2}+\\gamma\\frac{dx}{dt}+kx-F_0 cos(\\omega{t}+\\phi))^2$$\n",
    "\n",
    "For each training point, we calculate this physics-informed loss and take the average over all training sets.\n",
    "\n",
    "Additionally, we introduce a loss term called the \"trivial killer.\" In some cases (e.g., when the driven force is zero), the network may tend to output a straight line. While this conforms to physics at the time when the oscillator is at the origin and not oscillating, the residual loss will also remain zero.\n",
    "\n",
    "It can sometimes be challenging for initial condition constraints to address this issue. Therefore, we introduce the trivial killer loss, which calculates the first and second derivatives of the network's output and adds them together as a loss term. This helps prevent the network's output from converging to local extrema.\n",
    "\n",
    "After a certain point, when the network's output starts to oscillate, we must remove the trivial killer loss. This is because the actual solution will contain turning points (local extrema), and the trivial killer loss will hinder the network from learning the correct solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sin activation function class \n",
    "# uses for sin function as a neural network activation function\n",
    "# GOOGLE for SIREN network for details.\n",
    "\n",
    "class SinActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SinActivation, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sin(10*x)\n",
    "\n",
    "# Neural network class implement the SIREN network\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons, regularization_param, regularization_exp, retrain_seed):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = SinActivation()\n",
    "        self.regularization_param = regularization_param\n",
    "        # Regularization exponent\n",
    "        self.regularization_exp = regularization_exp\n",
    "        # Random seed for weight initialization\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def regularization(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.regularization_exp)\n",
    "        return self.regularization_param * reg_loss\n",
    "\n",
    "\n",
    "# Implement the PINN class for the driven oscillator problem\n",
    "class DrivenOscillatorPinns:\n",
    "    def __init__(self, n_int_):\n",
    "        self.n_int = n_int_\n",
    "\n",
    "        # Initial condition to solve driven oscillator\n",
    "        self.initial_x = 3.0\n",
    "        self.initial_v = 0.0\n",
    "        self.init_cond = torch.tensor([self.initial_x,self.initial_v])\n",
    "        \n",
    "        # System parameters\n",
    "        self.k = 50\n",
    "        self.mass = 0.5\n",
    "        self.c = 1\n",
    "        \n",
    "        # System parameters (Driven force)\n",
    "        self.omega_d = 2\n",
    "        self.phid = 0 \n",
    "        self.F_o = 50\n",
    "        \n",
    "        # Loss weights\n",
    "        self.initial_weight  = 1\n",
    "        self.residual_weight = 5\n",
    "        self.trivial_killer_weight = 1\n",
    "        \n",
    "        # Extrema of the solution domain (t) in [0,5]\n",
    "        self.domain_extrema = torch.tensor([[0, 12]])  \n",
    "        \n",
    "        # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=1)\n",
    "        # Initial training set\n",
    "        self.training_set_int = self.assemble_datasets()\n",
    "\n",
    "        # F Dense NN to approximate the solution of the underlying heat equation\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1,\n",
    "                                            n_hidden_layers=2,\n",
    "                                            neurons=100,\n",
    "                                            regularization_param=0.,\n",
    "                                            regularization_exp=0.,\n",
    "                                            retrain_seed=5).to(device)\n",
    "    \n",
    "    # Function driving_term to compute the driving term at time t\n",
    "    def driving_term(self, t):\n",
    "        return self.F_o*torch.cos(self.omega_d*t + self.phid)\n",
    "    # Function driven_oscillator to compute the analytical solution of the driven oscillator at time t\n",
    "    def driven_oscillator(self,t):\n",
    "        w0 = np.sqrt(self.k/self.mass)\n",
    "        gamma = self.c/2/self.mass\n",
    "        wprime = np.sqrt(w0**2 - gamma**2)\n",
    "        \n",
    "        print(f\"wprime = {wprime}\")\n",
    "        A = self.F_o / self.mass / np.sqrt((w0**2 - self.omega_d**2)**2 + 4 * gamma**2 * self.omega_d**2 )\n",
    "        \n",
    "        print(self.k, self.mass, self.omega_d**2)\n",
    "        phi = np.arctan(self.c * self.omega_d / (self.k - self.mass * self.omega_d**2)) - self.phid\n",
    "        phih = np.arctan(wprime * (self.initial_x - A * np.cos(phi)) / (self.initial_v + gamma * (self.initial_x - A * np.cos(phi)) - A * self.omega_d * np.sin(phi) ) )\n",
    "        \n",
    "        Ah = (self.initial_x - A * np.cos(phi)) / np.sin(phih)\n",
    "        \n",
    "        x = Ah * np.exp(-gamma * t) * np.sin(wprime * t + phih) + A * np.cos(self.omega_d * t - phi)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
    "    # to a tensor whose values are between the domain extrema\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[0][1] - self.domain_extrema[0][0]) + self.domain_extrema[0][0]\n",
    "\n",
    "    # Function to add interior points to the training set\n",
    "    def add_interior_points(self):\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        return input_int.to(device), output_int.to(device)\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_int, output_int = self.add_interior_points() # S_int\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "\n",
    "        return training_set_int\n",
    "    \n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
    "    def initial_velocity(self,init_t):\n",
    "        init_t.requires_grad = True\n",
    "        pred_x_init = self.approximate_solution(init_t)\n",
    "        pred_v_init = torch.autograd.grad(pred_x_init.sum(), init_t, create_graph=True)[0]\n",
    "        \n",
    "        return pred_v_init\n",
    "    # Function returns the predicted initial position by the network\n",
    "    # This function is used to compute the initial loss\n",
    "    def initial_position(self,init_t):\n",
    "        pred_x_init = self.approximate_solution(init_t)\n",
    "        return pred_x_init\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_int ,verbose=True):\n",
    "        # Compute the predicted initial position and velocity\n",
    "        init_t = torch.zeros(1).to(device)\n",
    "        pred_init_position = self.initial_position(init_t)\n",
    "        pred_init_velocity = self.initial_velocity(init_t)\n",
    "        # Compute the predicted solution and its gradient w.r.t. t\n",
    "        inp_train_int.requires_grad = True\n",
    "        u = self.approximate_solution(inp_train_int)\n",
    "        # Auto differentiation, use to compute the first and second derivatives of the network at each point of the training set\n",
    "        grad_u_t = torch.autograd.grad(u.sum(), inp_train_int, create_graph=True)[0]\n",
    "        grad_u_tt = torch.autograd.grad(grad_u_t.sum(), inp_train_int, create_graph=True)[0]\n",
    "        # Compute the residual of the ODE\n",
    "        residual = self.mass*grad_u_tt + self.c*grad_u_t + self.k*u  - self.driving_term(inp_train_int) \n",
    "        # Compute the loss from initial condition\n",
    "        init_position_error = self.init_cond[0] - pred_init_position\n",
    "        init_velocity_error = self.init_cond[1] - pred_init_velocity\n",
    "        loss_initial_position = torch.mean(init_position_error ** 2)\n",
    "        loss_initial_velocity = torch.mean(init_velocity_error ** 2)\n",
    "        # Compute the loss from the ODE (Mean square error)\n",
    "        loss_int = torch.mean(residual ** 2) \n",
    "        loss_tb = loss_initial_position + loss_initial_velocity\n",
    "        # Trivial killer loss, This loss is used to kill trivial solutions\n",
    "        # We did not use it in this example\n",
    "        loss_trivial = (torch.reciprocal(torch.mean(u**2)) + torch.reciprocal(torch.mean(grad_u_t**2)) + torch.reciprocal(torch.mean(grad_u_tt**2)))\n",
    "        # Compute the total loss (weighted sum of the three losses)\n",
    "        loss = loss_tb * self.initial_weight  + loss_int * self.residual_weight + loss_trivial * self.trivial_killer_weight\n",
    "        # Debug print\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 8), \n",
    "                        \"| PDE Loss: \", round(loss_int.item(), 8), \n",
    "                        \"| Initial Loss: \", round(loss_tb.item(), 8),\n",
    "                        \"| Trivial Loss: \", round(loss_trivial.item(), 8))\n",
    "        return loss \n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "            \n",
    "            for _, (inp_train_int, u_train_int) in enumerate(self.training_set_int):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_int ,verbose=verbose)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "\n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Minimum Loss: ', min(history))\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of interior points\n",
    "n_int = 3000\n",
    "pinn = DrivenOscillatorPinns(n_int)\n",
    "# initialize the training set\n",
    "input_int_, output_int_ = pinn.add_interior_points()\n",
    "# Storage the training history (Loss)\n",
    "hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the predicted solution before training\n",
    "# For transcient state of the driven oscillator, we tooks more points\n",
    "# since the change is more rapid in the transcient state\n",
    "transcient_1 = torch.linspace(0, 1, 50).reshape(-1,1)\n",
    "transcient_2 = torch.linspace(1, 2, 20).reshape(-1,1)\n",
    "transcient   = torch.cat((transcient_1, transcient_2), 0)\n",
    "# For steady state of the driven oscillator, we tooks less points\n",
    "steady = torch.linspace(2, 12, 100).reshape(-1,1)\n",
    "# combine transcient and steady time points\n",
    "time_points = torch.cat((transcient, steady), 0)\n",
    "output = pinn.approximate_solution(time_points)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time_points.detach().numpy()[:,0], y=pinn.driven_oscillator(time_points).detach().numpy()[:,0], mode='markers', name='<b>Analytical Solution</b>', marker=dict(color='rgb(115, 187, 222)',size=7,symbol=\"star-diamond-open\")))\n",
    "fig.add_trace(go.Scatter(x=time_points.detach().numpy()[:,0], y=output.detach().numpy()[:,0], mode='markers', name='<b>PINN          Solution</b>', marker=dict(color='rgb(255, 169, 127)',size=6)))\n",
    "fig.update_layout(xaxis_title=r'$\\text{time}{(s)}$', yaxis_title=r'$\\text{position}{(m)}$', template='plotly_dark',margin=dict(l=0, r=0, t=20, b=0),legend=dict(\n",
    "    yanchor=\"top\",font=dict(\n",
    "            size=16,\n",
    "            color=\"white\"\n",
    "        ),\n",
    "    y=0.97,\n",
    "    xanchor=\"right\",\n",
    "    x=0.97,\n",
    "    bordercolor=\"white\",\n",
    "    borderwidth=1\n",
    "))\n",
    "fig.write_image('output/training_proc/Before_trainning.png',scale=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup learning rate and optimizer\n",
    "l_rate = 0.001\n",
    "optimizer_ADAM = optim.Adam(pinn.approximate_solution.parameters(),\n",
    "                                lr=float(l_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it is necessary to lower the learning rate\n",
    "# Run the following code\n",
    "optimizer_ADAM.param_groups[0]['lr'] *= 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "for i in range(9):\n",
    "\n",
    "    total_epochs += epoch\n",
    "\n",
    "    # set loss weights to train the network\n",
    "    pinn.initial_weight        = 600.0\n",
    "    pinn.residual_weight       = 10000.0\n",
    "    pinn.trivial_killer_weight = 0.0\n",
    "\n",
    "    hist += pinn.fit(num_epochs=epoch,\n",
    "                    optimizer=optimizer_ADAM,\n",
    "                    verbose=True)\n",
    "    # transcient_1 = torch.linspace(0, 1, 50).reshape(-1,1)\n",
    "    # transcient_2 = torch.linspace(1, 2, 20).reshape(-1,1)\n",
    "    # transcient   = torch.cat((transcient_1, transcient_2), 0)\n",
    "    # steady = torch.linspace(2, 12, 100).reshape(-1,1)\n",
    "    # # combine transcient and steady time points\n",
    "    # time_points = torch.cat((transcient, steady), 0)\n",
    "    # output = pinn.approximate_solution(time_points)\n",
    "\n",
    "    # fig = go.Figure()\n",
    "    # fig.add_trace(go.Scatter(x=time_points.detach().numpy()[:,0], y=pinn.driven_oscillator(time_points).detach().numpy()[:,0], mode='markers', name='<b>Analytical Solution</b>', marker=dict(color='rgb(115, 187, 222)',size=7,symbol=\"star-diamond-open\")))\n",
    "    # fig.add_trace(go.Scatter(x=time_points.detach().numpy()[:,0], y=output.detach().numpy()[:,0], mode='markers', name='<b>PINN          Solution</b>', marker=dict(color='rgb(255, 169, 127)',size=6)))\n",
    "    # #set marker size\n",
    "    # fig.update_xaxes(tickfont = dict(size=20),titlefont=dict(size=20))\n",
    "    # fig.update_yaxes(tickfont = dict(size=20),titlefont=dict(size=20))\n",
    "    # fig.update_layout(xaxis_title='time (s)', yaxis_title='position (m)', template='plotly_dark',margin=dict(l=0, r=0, t=20, b=0),legend=dict(\n",
    "    #     yanchor=\"top\",font=dict(\n",
    "    #             size=16,\n",
    "    #             color=\"white\"\n",
    "    #         ),\n",
    "    #     y=0.97,\n",
    "    #     xanchor=\"right\",\n",
    "    #     x=0.97,\n",
    "    #     bordercolor=\"white\",\n",
    "    #     borderwidth=1\n",
    "    # ))\n",
    "    # fig.write_image(f'output/training_proc/epoch{total_epochs}_lr{l_rate}.png',scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the loss function over epoches\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=hist,mode='markers', name='Loss', marker=dict(color='rgb(115, 187, 222)',size=7)))\n",
    "# set dark theme\n",
    "# set x-axis to log scale\n",
    "fig.update_layout(xaxis_title='epochs', yaxis_title='value', \n",
    "                  template='plotly_dark',margin=dict(l=0, r=0, t=20, b=0),\n",
    "                  legend=dict(\n",
    "                  yanchor=\"top\",font=dict(\n",
    "                        size=24,\n",
    "                        color=\"white\"\n",
    "                    ),\n",
    "               y=0.97,\n",
    "               xanchor=\"right\",\n",
    "               x=0.97,\n",
    "               bordercolor=\"white\",\n",
    "               borderwidth=1\n",
    "            ),\n",
    "                showlegend=True\n",
    "                )\n",
    "fig.update_xaxes(type=\"log\",tickfont = dict(size=28),titlefont=dict(size=32),dtick = 1,tickformat='.1e')\n",
    "fig.update_yaxes(type=\"log\",tickfont = dict(size=28),titlefont=dict(size=32),dtick = 1,tickformat='.1e')\n",
    "fig.write_image('output/Loss_function.png',scale=5,width= 1600, height= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the predicted initial velocity and position\n",
    "init_t = torch.zeros(1)\n",
    "pred_init_position = pinn.initial_position(init_t)\n",
    "pred_init_velocity = pinn.initial_velocity(init_t)\n",
    "print(\"Initial position: \", pred_init_position.item())\n",
    "print(\"Initial velocity: \", pred_init_velocity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly draw 10000 points to calculate the error between the analytical solution and the PINN solution\n",
    "inputs = pinn.soboleng.draw(10000)\n",
    "inputs = pinn.convert(inputs)\n",
    "output = pinn.approximate_solution(inputs)\n",
    "err_square = (pinn.driven_oscillator(inputs).detach().numpy()[:,0]-output.detach().numpy()[:,0])**2\n",
    "err_mean = np.mean(err_square)\n",
    "err_std = np.std(err_square)\n",
    "print(\"Mean squared error: \", err_mean)\n",
    "print(\"Standard deviation of squared error: \", err_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
